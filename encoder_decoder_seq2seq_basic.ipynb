{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbd8NuEgS4Eh",
        "outputId": "3d6c07c6-6265-4912-b3dc-ffeae3e62b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from platform import python_version\n",
        "import  matplotlib,torch,torchtext\n",
        "!pip install portalocker>=2.0.0\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "from torchtext.vocab import vocab\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv('/content/samsum-train.csv',engine='python',encoding='utf-8', error_bad_lines=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P2YCD1RS_8f",
        "outputId": "83b81608-8ab2-4392-f2a2-c661433b4408"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b5e891d57e74>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  dataframe = pd.read_csv('/content/samsum-train.csv',engine='python',encoding='utf-8', error_bad_lines=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(text):\n",
        "    # step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n",
        "    # step 2: use lowercase for all text to keep symmetry\n",
        "    # step 3: extract emoticons. keep them as they are important sentiment signals\n",
        "    # step 4: remove punctuation marks\n",
        "    # step 5: put back emoticons\n",
        "    # step 6: generate word tokens\n",
        "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
        "    text = text.lower()\n",
        "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
        "    text = re.sub(\"[\\W]+\", \" \", text)\n",
        "    text = text + \" \".join(emoticons).replace(\"-\", \"\")\n",
        "    tokenized = text.split()\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "ohPN3sz3TMkB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_tokens=tokenizer(dataframe.dialogue[0])\n",
        "example_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvgbGwZrULvV",
        "outputId": "7660c485-ef54-4843-801e-7070f22ae49d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amanda',\n",
              " 'i',\n",
              " 'baked',\n",
              " 'cookies',\n",
              " 'do',\n",
              " 'you',\n",
              " 'want',\n",
              " 'some',\n",
              " 'jerry',\n",
              " 'sure',\n",
              " 'amanda',\n",
              " 'i',\n",
              " 'll',\n",
              " 'bring',\n",
              " 'you',\n",
              " 'tomorrow',\n",
              " ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_counts = Counter()\n",
        "token_counts.update(example_tokens)\n",
        "token_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ej5hLVSVXaZ",
        "outputId": "95980ed7-832d-418e-8216-c3bcfa858e73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'amanda': 2,\n",
              "         'i': 2,\n",
              "         'baked': 1,\n",
              "         'cookies': 1,\n",
              "         'do': 1,\n",
              "         'you': 2,\n",
              "         'want': 1,\n",
              "         'some': 1,\n",
              "         'jerry': 1,\n",
              "         'sure': 1,\n",
              "         'll': 1,\n",
              "         'bring': 1,\n",
              "         'tomorrow': 1,\n",
              "         ':)': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_by_freq_tuples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLxkdEZjVdBJ",
        "outputId": "33b0d328-c47c-4ba3-e7bf-f3a2372912ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('amanda', 2),\n",
              " ('i', 2),\n",
              " ('you', 2),\n",
              " ('baked', 1),\n",
              " ('cookies', 1),\n",
              " ('do', 1),\n",
              " ('want', 1),\n",
              " ('some', 1),\n",
              " ('jerry', 1),\n",
              " ('sure', 1),\n",
              " ('ll', 1),\n",
              " ('bring', 1),\n",
              " ('tomorrow', 1),\n",
              " (':)', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "ordered_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z58b6-RNVgfH",
        "outputId": "3c9cf292-dde3-426d-a4e2-b06369ad7716"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('amanda', 2),\n",
              "             ('i', 2),\n",
              "             ('you', 2),\n",
              "             ('baked', 1),\n",
              "             ('cookies', 1),\n",
              "             ('do', 1),\n",
              "             ('want', 1),\n",
              "             ('some', 1),\n",
              "             ('jerry', 1),\n",
              "             ('sure', 1),\n",
              "             ('ll', 1),\n",
              "             ('bring', 1),\n",
              "             ('tomorrow', 1),\n",
              "             (':)', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ordered_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc3M8razVjhA",
        "outputId": "12f17d02-c2f3-4e5f-838b-99c3998b79fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vb = vocab(ordered_dict)\n",
        "vb.get_stoi()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5L5YJcDVnbS",
        "outputId": "a32dbb52-5fe3-430b-d48a-6bd1b5f34ba5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amanda': 0,\n",
              " 'i': 1,\n",
              " ':)': 13,\n",
              " 'you': 2,\n",
              " 'baked': 3,\n",
              " 'cookies': 4,\n",
              " 'tomorrow': 12,\n",
              " 'do': 5,\n",
              " 'some': 7,\n",
              " 'want': 6,\n",
              " 'jerry': 8,\n",
              " 'sure': 9,\n",
              " 'll': 10,\n",
              " 'bring': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['dialogue'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myWsQwFfX7IL",
        "outputId": "94cb2895-a6c9-4167-cc1c-2c976acfee64"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = dataframe.dropna(subset=['dialogue'])"
      ],
      "metadata": {
        "id": "H6yfuaPwX7Ox"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre-processing for dialogue"
      ],
      "metadata": {
        "id": "RvSjOaIbZqN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: convert reviews into tokens\n",
        "# step 2: find frequency of tokens\n",
        "\n",
        "token_counts_dialogue = Counter()\n",
        "\n",
        "for i in range(0,len(dataframe)):\n",
        "    tokens_dia = tokenizer(dataframe['dialogue'].iloc[i])\n",
        "    token_counts_dialogue.update(tokens_dia)\n",
        "\n",
        "\n",
        "print('dialogue vocab size:', len(token_counts_dialogue))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYXEu9W2Vp_U",
        "outputId": "c1268daa-57ed-4921-8ec9-cd4ab13a3724"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dialogue vocab size: 29629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3: sort the token based on their frequency\n",
        "# step 4: put the sorted tokens in OrderedDict\n",
        "# step 5: convert token to integers using vocab object\n",
        "\n",
        "sorted_by_freq_tuples = sorted(token_counts_dialogue.items(), key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "vb = vocab(ordered_dict)\n",
        "\n",
        "vb.insert_token(\"<pad>\", 0)    # Padding token\n",
        "vb.insert_token(\"<unk>\", 1)    # Unknown token\n",
        "vb.insert_token(\"<start>\", 2)  # Start of sequence token\n",
        "vb.insert_token(\"<end>\", 3)    # End of sequence token\n",
        "\n",
        "vb.set_default_index(1)  # Default index for unknown words\n",
        "\n",
        "# print some token indexes from vocab\n",
        "for token in [\"<start>\", \"this\", \"is\", \"<end>\"]:\n",
        "    print(token, \" --> \", vb[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgLO3TBsW2IS",
        "outputId": "f338f842-f4bf-490f-8629-1b49d634cdbc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start>  -->  2\n",
            "this  -->  40\n",
            "is  -->  16\n",
            "<end>  -->  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing for Summary"
      ],
      "metadata": {
        "id": "F6Cl25gKZxSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: convert reviews into tokens\n",
        "# step 2: find frequency of tokens\n",
        "\n",
        "token_counts_summary = Counter()\n",
        "\n",
        "for i in range(0,len(dataframe)):\n",
        "    tokens_summary = tokenizer(dataframe['summary'].iloc[i])\n",
        "    token_counts_summary.update(tokens_summary)\n",
        "\n",
        "\n",
        "print('dialogue Summary size:', len(token_counts_summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3vPkOxdYR13",
        "outputId": "00cae2c9-6a3c-4834-edad-e54ea674fa9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dialogue Summary size: 16057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3: sort the token based on their frequency\n",
        "# step 4: put the sorted tokens in OrderedDict\n",
        "# step 5: convert token to integers using vocab object\n",
        "\n",
        "sorted_by_freq_tuples_summary = sorted(token_counts_summary.items(), key=lambda x: x[1], reverse=True)\n",
        "ordered_dict_summary = OrderedDict(sorted_by_freq_tuples_summary)\n",
        "\n",
        "vb_summary = vocab(ordered_dict_summary)\n",
        "\n",
        "vb_summary.insert_token(\"<pad>\", 0)    # Padding token\n",
        "vb_summary.insert_token(\"<unk>\", 1)    # Unknown token\n",
        "vb_summary.insert_token(\"<start>\", 2)  # Start of sequence token\n",
        "vb_summary.insert_token(\"<end>\", 3)    # End of sequence token\n",
        "\n",
        "vb_summary.set_default_index(1)  # Default index for unknown words\n",
        "\n",
        "# print some token indexes from vocab\n",
        "for token in [\"<start>\", \"this\", \"is\", \"<end>\"]:\n",
        "    print(token, \" --> \", vb[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_abr-fj8aFSp",
        "outputId": "6979cd64-87d1-47f5-a425-9d1e92cea3b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start>  -->  2\n",
            "this  -->  40\n",
            "is  -->  16\n",
            "<end>  -->  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inline lambda functions for text and label precessing\n",
        "\"\"\"dialogue_pipeline = lambda x: [vb[token] for token in tokenizer(x)]\n",
        "summary_pipeline = lambda x: [vb_summary[token] for token in tokenizer(x)]\"\"\"\n",
        "\n",
        "dialogue_pipeline = lambda x: [vb[\"<start>\"]] + [vb[token] for token in tokenizer(x)] + [vb[\"<end>\"]]\n",
        "summary_pipeline = lambda x: [vb_summary[\"<start>\"]] + [vb_summary[token] for token in tokenizer(x)] + [vb_summary[\"<end>\"]]\n"
      ],
      "metadata": {
        "id": "nOUoyM_SaW4e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBWcstAEbck-",
        "outputId": "9fd8f6cf-e312-4e4b-868c-fefbb23fb54c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'dialogue', 'summary'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.dialogue[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eX-Ym5pmbklI",
        "outputId": "fa09f083-db1d-46b4-a47c-136f12011ee6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue_pipeline(dataframe.dialogue[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tSDTQb4bm-b",
        "outputId": "a99e18b9-026b-4839-f672-013eec20b354"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 421, 4, 3947, 1830, 30, 5, 86, 61, 611, 62, 421, 4, 33, 257, 5, 121, 58, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.summary[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Mb7haKRgbxzO",
        "outputId": "c7321711-04f9-4648-c188-9332f6265523"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Amanda baked cookies and will bring Jerry some tomorrow.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_pipeline(dataframe.summary[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000asAdobqRU",
        "outputId": "191b2419-cd4b-4105-f96c-5c3285220d09"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 366, 2238, 967, 6, 9, 93, 546, 44, 41, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_vect_dialogue = []\n",
        "\n",
        "for i in range(0,len(dataframe['dialogue'])):\n",
        "  out = dialogue_pipeline(dataframe.dialogue.iloc[i])\n",
        "  train_data_vect_dialogue.append(out)"
      ],
      "metadata": {
        "id": "m4R0LLNrbuIM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_vect_summary = []\n",
        "\n",
        "for i in range(0,len(dataframe['summary'])):\n",
        "  out = summary_pipeline(dataframe.summary.iloc[i])\n",
        "  train_data_vect_summary.append(out)"
      ],
      "metadata": {
        "id": "LtY1wnMZb_AS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_vect_dialogue[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NFMzcvMcCYo",
        "outputId": "9a47221e-0b62-4a3d-fc69-604ca4f2e823"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 421, 4, 3947, 1830, 30, 5, 86, 61, 611, 62, 421, 4, 33, 257, 5, 121, 58, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_vect_summary[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkoVSAj6cFfS",
        "outputId": "df479105-ebaf-415c-9037-55079db4fc0b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 366, 2238, 967, 6, 9, 93, 546, 44, 41, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataframe = pd.read_csv('/content/samsum-test.csv',engine='python',encoding='utf-8', error_bad_lines=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzOSabNAcH3Q",
        "outputId": "1e3e91e3-4091-4d9e-bdb1-bf4c1966ce02"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-2e08d2132617>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  test_dataframe = pd.read_csv('/content/samsum-test.csv',engine='python',encoding='utf-8', error_bad_lines=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataframe = pd.read_csv('/content/samsum-validation.csv',engine='python',encoding='utf-8', error_bad_lines=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjPoayH6cjyD",
        "outputId": "22f73ca8-7b37-4f7e-8dbc-fc745091c4dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-aa3a2d478b16>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  validation_dataframe = pd.read_csv('/content/samsum-validation.csv',engine='python',encoding='utf-8', error_bad_lines=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [(dialogue, summary) for dialogue, summary in zip(dataframe['dialogue'], dataframe['summary'])]"
      ],
      "metadata": {
        "id": "Fsz7gb5HcmDq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = [(dialogue, summary) for dialogue, summary in zip(validation_dataframe['dialogue'], validation_dataframe['summary'])]"
      ],
      "metadata": {
        "id": "Qolr34glco4y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [(dialogue, summary) for dialogue, summary in zip(test_dataframe['dialogue'], test_dataframe['summary'])]"
      ],
      "metadata": {
        "id": "Uiy5Ihb-crK6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gQzUimSctg7",
        "outputId": "111743f7-d26e-481e-fd6d-0df20fe2fc99"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
              " 'Amanda baked cookies and will bring Jerry some tomorrow.')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECi4Tz72cv7S",
        "outputId": "cecefd56-46ba-48a5-b640-3af33aafd083"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"A: Hi Tom, are you busy tomorrowâ€™s afternoon?\\r\\nB: Iâ€™m pretty sure I am. Whatâ€™s up?\\r\\nA: Can you go with me to the animal shelter?.\\r\\nB: What do you want to do?\\r\\nA: I want to get a puppy for my son.\\r\\nB: That will make him so happy.\\r\\nA: Yeah, weâ€™ve discussed it many times. I think heâ€™s ready now.\\r\\nB: Thatâ€™s good. Raising a dog is a tough issue. Like having a baby ;-) \\r\\nA: I'll get him one of those little dogs.\\r\\nB: One that won't grow up too big;-)\\r\\nA: And eat too much;-))\\r\\nB: Do you know which one he would like?\\r\\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\\r\\nB: I bet you had to drag him away.\\r\\nA: He wanted to take it home right away ;-).\\r\\nB: I wonder what he'll name it.\\r\\nA: He said heâ€™d name it after his dead hamster â€“ Lemmy  - he's  a great Motorhead fan :-)))\",\n",
              " 'A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. ')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAjpp3lAcyRd",
        "outputId": "33925a5a-7e88-44bb-b538-be90d354203c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ðŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\",\n",
              " \"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCnomMKSc04N",
        "outputId": "7bbcd0ee-08cb-4327-c7db-71fc9b22e5d0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "# a function to apply pre-processing steps at a batch level\n",
        "import torch.nn as nn\n",
        "\n",
        "def collate_batch(batch):\n",
        "    dialogue_list, summary_list, length_dialogue, length_summary = [], [], [], []\n",
        "\n",
        "    # iterate over all reviews in a batch\n",
        "    for _dialogue, _summary in batch:\n",
        "        # dialogue preprocessing\n",
        "        dialogue_processing = torch.tensor(dialogue_pipeline(_dialogue), dtype=torch.int64)\n",
        "        dialogue_list.append(dialogue_processing)\n",
        "        length_dialogue.append(dialogue_processing.size(0))\n",
        "        # summary preprocessing\n",
        "        summary_processing = torch.tensor(summary_pipeline(_summary), dtype=torch.int64)\n",
        "        summary_list.append(summary_processing)\n",
        "\n",
        "        # store the length of processed text\n",
        "        # this will come handy in future when we want to know the original size of a text (without padding)\n",
        "        length_summary.append(summary_processing.size(0))\n",
        "\n",
        "    length_dialogue = torch.tensor(length_dialogue)\n",
        "    length_summary = torch.tensor(length_summary)\n",
        "    # pad the processed reviews to make their lengths consistant\n",
        "    padded_dialogue_list = nn.utils.rnn.pad_sequence(\n",
        "        dialogue_list, batch_first=True)\n",
        "    padded_summary_list = nn.utils.rnn.pad_sequence(\n",
        "        summary_list, batch_first=True)\n",
        "\n",
        "    # return\n",
        "    # 1. a list of processed and padded review texts\n",
        "    # 2. a list of processed labels\n",
        "    # 3. a list of review text original lengths (before padding)\n",
        "    return padded_dialogue_list.to(device), padded_summary_list.to(device), length_dialogue.to(device), length_summary.to(device)"
      ],
      "metadata": {
        "id": "pRqH2Py9c4AU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(\n",
        "    train_data, batch_size=32, shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "dialogue_batch,summary_batch,length_dialogue_batch, length_summary_batch = next(iter(dataloader))\n",
        "\n",
        "print(\"summary_batch.shape: \", summary_batch.shape)\n",
        "print(\"dialogue_batch.shape: \", dialogue_batch.shape)\n",
        "print(\"length_dialogue_batch: \", length_dialogue_batch)\n",
        "print(\"length_summary_batch: \", length_summary_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbnvZGDbdiUI",
        "outputId": "9db055f9-7306-4046-9b5f-31f618ae8750"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary_batch.shape:  torch.Size([32, 61])\n",
            "dialogue_batch.shape:  torch.Size([32, 371])\n",
            "length_dialogue_batch:  tensor([ 19,  20, 105,  31, 192,  83, 371, 168,  51, 134,  62,  19,  23, 116,\n",
            "        108, 141,  58,  78,  46,  42, 260, 229,  17, 269,  49,  84,  55, 119,\n",
            "        114, 111, 334,  88], device='cuda:0')\n",
            "length_summary_batch:  tensor([11, 12, 16, 20, 29, 29, 45, 34, 16, 41, 17, 16,  7, 20, 20, 20, 37, 15,\n",
            "        19, 18, 16, 60, 13, 35, 19, 22, 20, 34, 38, 16, 61, 25],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "valid_dl = DataLoader(\n",
        "    validation_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "bwKi3NfQooRO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "z77wXvWjebl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils"
      ],
      "metadata": {
        "id": "O4LZss4D6LM8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        # src = [batch size, src len]\n",
        "        # src_len = [batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # Ensure src_len is on CPU and of type int64\n",
        "        src_len = src_len.to('cpu').long()\n",
        "        # Pack sequence\n",
        "        packed_embedded = rnn_utils.pack_padded_sequence(embedded, src_len, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack sequence\n",
        "        outputs, _ = rnn_utils.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "\n",
        "        # outputs = [batch size, src len, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "VmBMBzV26DqC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DECODER"
      ],
      "metadata": {
        "id": "R7mGEQVP6NTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input = [batch size]\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # cell = [n layers, batch size, hid dim]\n",
        "\n",
        "        # Convert input to [batch size, 1] (as we process one word at a time)\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        # embedded = [batch size, 1, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        # output = [batch size, seq len, hid dim]\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "\n",
        "        # prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "oWpEEphSfKG9"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "ZqM6gRvmgSNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        # src = [batch size, src len]\n",
        "        # src_len = [batch size]\n",
        "        # trg = [batch size, trg len]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # encode the source sequence\n",
        "        hidden, cell = self.encoder(src, src_len)\n",
        "\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "PYwgDMnZgBhY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming your model, vocabulary, and other parameters are already defined\n",
        "\n",
        "# Initialize the loss function with ignore_index set to the padding token index\n",
        "PAD_IDX = vb_summary[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Initialize the optimizer (e.g., Adam)\n",
        "optimizer = optim.Adam(model.parameters())\n"
      ],
      "metadata": {
        "id": "KcL5gVoBiJuI"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vb)\n",
        "OUTPUT_DIM = len(vb_summary)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "LB5joXVZ6-U7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Encoder and Decoder\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "# Instantiate the Seq2Seq model\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "metadata": {
        "id": "4obs9Ck-7KM7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cDeTLNNixG4",
        "outputId": "4943497a-e4f0-4145-9725-3014ef08ff5f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(29633, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(16061, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=16061, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, trg, src_len, trg_len = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, src_len, trg)\n",
        "\n",
        "        # trg = [batch size, trg len]\n",
        "        # output = [batch size, trg len, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        # Reshape the outputs and targets so they are [batch size * trg len, output dim]\n",
        "        output = output.reshape(-1, output_dim)\n",
        "        trg = trg.reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient to prevent it from exploding\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "6JmAM7prnQrj"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, trg, src_len, trg_len = batch\n",
        "\n",
        "            output = model(src, src_len, trg, 0)  # turn off teacher forcing\n",
        "\n",
        "            # trg = [batch size, trg len]\n",
        "            # output = [batch size, trg len, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            # Reshape the outputs and targets so they are [batch size * trg len, output dim]\n",
        "            output = output.reshape(-1, output_dim)\n",
        "            trg = trg.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "MdabKkWkoTE6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "YcGZ7TLblTsI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch"
      ],
      "metadata": {
        "id": "UQnhpdGfs6Nd"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "n_epochs = 10\n",
        "clip = 1"
      ],
      "metadata": {
        "id": "zbbQWoPdlW86"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(n_epochs), desc = 'processing'):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_dl, optimizer, criterion, clip)\n",
        "    valid_loss = evaluate(model, valid_dl, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUjyuu6A7l8D",
        "outputId": "89f8387c-b923-481a-d5b3-b7e752fe4f1d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "processing:  10%|â–ˆ         | 1/10 [02:23<21:35, 143.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 23s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16289.824\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  20%|â–ˆâ–ˆ        | 2/10 [04:47<19:10, 143.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Time: 2m 23s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16290.121\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [07:11<16:47, 143.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 2m 24s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16289.801\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [09:36<14:24, 144.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 2m 24s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16290.515\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [12:00<12:00, 144.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 2m 24s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16289.269\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [14:23<09:35, 143.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 2m 23s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16290.060\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [16:47<07:11, 143.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Time: 2m 23s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16289.856\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [19:12<04:48, 144.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 2m 25s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16289.985\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rprocessing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [21:37<02:24, 144.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Time: 2m 24s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16289.145\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [24:01<00:00, 144.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 2m 23s\n",
            "\tTrain Loss: 9.698 | Train PPL: 16289.673\n",
            "\t Val. Loss: 9.696 |  Val. PPL: 16256.831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "7tTR_iMF7mDU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(model, test_dl, criterion)"
      ],
      "metadata": {
        "id": "hrB2whStMPiN"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHDsMaZ4MPlO",
        "outputId": "50634db8-33f3-4579-c8ba-ff4c61dafa5d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.69612741470337"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzPIfKUjMPos"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}